## Llama 2 Inroduction

The Llama 2 architecture took its inspiration from the **transformer architecture** but differed from other instances of second generation transformers like gpt2, gpt3 and  deep-seek etc.


## Structural component of te Llama 2 block

1. ** Token Embeddings **  : The input data are tokenised with byte pair encoding algorithm for computational efficiency

2.** Pre-nomralisation ** : The  input of each transformer sub-layer is normalised using Root mean square normalisation to improve training stability. The sub-layer of the transformer consist of :
      a. ** Input layer**
      b. ** Self attention layer (GQA + RoPE) **
      c. ** Feed forward layer **
      d. ** Output layer


3. ** Grouped Query Masked Self-Attention (GQA + MSA)** : This te communication layer of the llama 2 architecture, it differs quite slihtly from th pt2 self attention mechanism because it shares key/value heads across query heads (i.e query has more head than key and value since they are both sharing heads) ultimately reducing computational bottlenecks and the masked self-attention  prevents future information leakage 

       Traditional multi-head attention assigns separate key, query, and value projections for each head.

      GQA reduces memory and compute cost by sharing key/value heads across multiple query heads.

      **This means:**

      ..Queries have more heads than keys/values.

      ..Keys/values are reused across queries, lowering redundancy.

      Benefits:

      Faster inference.

      Lower memory footprint.

      Comparable performance to full multi-head attention.

Combined with Masked Self-Attention (MSA), GQA ensures tokens cannot attend to future positions, preserving autoregressive training.



4. ** Rotary Embeddings ** : This can be seen as part of GQA since it is applied directly of the query and key to give it a rotary positional embedding completely different from absolute positional embedding as it takes into consideration te relative position of each tokens.

5. ** Feed Forward Network ** : If Attention is communication, the FFN is "thinking." This is a linear layer followed by a non-linearity ( SWiGLu). It acts as a **knowledge bank**, processing the information gathered during the attention phase.

6 ** Residual Network ** : We add the input of the block back to its output (). This allows gradients to flow through the network during backpropagation without vanishing, enabling us to train deeper models.

---

## 2. Data Dimensions & Tensors

We handle data in 4D or 3D tensors. For our implementation, the core shape is :

* **B (Batch):** 64 (Processing 64 sequences at once).
* **T (Time/Block Size):** 4096 (The maximum context length/history the model sees).
* **C (Channel/Embed Dim):** `n_embed` (The size of the vector representing a single token).
* **n_heads (number of heads ): ** (the number of heads is critical because it determines ow many independent attention patterns the model can learn simultaneously)
* **n_kv_heads :** 8 The n_kv_eads is critical because it directly controls the memory cost of generation

---

---

## 3. The Mechanics of Masked Self-Attention

Self-attention allows tokens to "vote" on which other tokens are relevant to them.

### The Mathematical Flow

We derive three vectors for every token using trainable weights :

1. **Query ():** What am I looking for?
2. **Key ():** What information do I contain?
3. **Value ():** If I am relevant, what information do I contribute?

**The Score Calculation:**
attention_weights= Softmax((Q@K.T) * (head_size)**-0.5) * V


* **Matrix Multiplication ():** This creates an "Affinity Matrix" of size , showing how much every token in the sequence relates to every other token.
* **Scaling:** We divide by  to keep the values small, ensuring the Softmax gradients stay healthy.
* **The Mask (Tril):** We apply a lower-triangular mask (`torch.tril`). This fills the upper triangle (the "future") with .
* **Softmax:** The  values become , and the remaining values are normalized to sum to 1. This creates a "weighted map" of the past.
* **Value Aggregation:** We multiply this map by  to get the final context-aware representation of the token.

---
## 4. The Mechanics of Root Mean Square Normalisation
 Root mean square normalisation ensures data stability , it is completely different from  layernorm as it is computationally efficient and removes the mean subtraction


 RMS= (mean_sq + epsilon) ** 0.5; Xnorm,i= Xi/(RMS)

 1. ** epsilon:**  helps to avoid zero division error 
 2 ** mean_sq/ Variance :** measures how far the value is from the mean varinace= x.pow(2)
